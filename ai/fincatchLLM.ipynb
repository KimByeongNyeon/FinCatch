{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE-ZTEbi00rR",
        "outputId": "64ed15b3-cc97-4d6e-a925-ca0311267a63",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (78.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.3.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Found existing installation: JPype1 1.4.1\n",
            "Uninstalling JPype1-1.4.1:\n",
            "  Successfully uninstalled JPype1-1.4.1\n",
            "Collecting JPype1==1.4.1\n",
            "  Using cached JPype1-1.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1==1.4.1) (24.2)\n",
            "Using cached JPype1-1.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (464 kB)\n",
            "Installing collected packages: JPype1\n",
            "Successfully installed JPype1-1.4.1\n"
          ]
        }
      ],
      "source": [
        "# 기본 패키지 설치 (추가: fastapi, uvicorn, nest_asynci)\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install pandas scikit-learn joblib fastapi uvicorn nest-asyncio konlpy optuna\n",
        "\n",
        "# JPype1 버전 고정 (konlpy 호환)\n",
        "!pip uninstall JPype1 -y\n",
        "!pip install JPype1==1.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7xMX4UJWGcf",
        "outputId": "47f02888-463f-4a3c-ba81-014d69b7cf86",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "clinfo is already the newest version (3.0.21.02.21-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Number of platforms                               0\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y clinfo\n",
        "!clinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDpXgIQOC2A3",
        "outputId": "26090340-b7fb-4204-ef56-694522ef0a7b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx7WFR6y04lN",
        "outputId": "100860a0-efc4-47c3-e3a6-1135b17a5e00",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [70.9 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,808 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,978 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,148 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,092 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,540 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,241 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,775 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,688 kB]\n",
            "Fetched 30.1 MB in 4s (6,851 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libboost-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-dev set to manually installed.\n",
            "libboost-filesystem-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-filesystem-dev set to manually installed.\n",
            "libboost-system-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-system-dev set to manually installed.\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n",
            "Submodule path 'external_libs/compute': checked out '36350b7de849300bd3d72a05d8bf890ca405a014'\n",
            "Submodule path 'external_libs/eigen': checked out '3147391d946bb4b6c68edd901f2add6ac1f31f8c'\n",
            "Submodule path 'external_libs/fast_double_parser': checked out '252029ddac664370bdda3f0761675785d92a1573'\n",
            "Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp': checked out 'd936052d32a5b7ca08b0199a6724724aea432309'\n",
            "Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion': checked out 'f4cb2384efa55dee0e6652f8674b05763441ab09'\n",
            "Submodule path 'external_libs/fmt': checked out '8303d140a1a11f19b982a9f664bbe59a1ccda3f4'\n",
            "M\texternal_libs/compute\n",
            "M\texternal_libs/eigen\n",
            "M\texternal_libs/fast_double_parser\n",
            "M\texternal_libs/fmt\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CUDA: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Found CUDA: /usr/local/cuda (found suitable version \"12.5\", minimum required is \"9.0\")\n",
            "-- CMAKE_CUDA_FLAGS: -Xcompiler=-fopenmp -Xcompiler=-fPIC -Xcompiler=-Wall -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_62,code=sm_62 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -O3 -lineinfo\n",
            "-- ALLFEATS_DEFINES: -DPOWER_FEATURE_WORKGROUPS=12;-DUSE_CONSTANT_BUF=0;-DENABLE_ALL_FEATURES\n",
            "-- FULLDATA_DEFINES: -DPOWER_FEATURE_WORKGROUPS=12;-DUSE_CONSTANT_BUF=0;-DENABLE_ALL_FEATURES;-DIGNORE_INDICES\n",
            "-- Performing Test MM_PREFETCH\n",
            "-- Performing Test MM_PREFETCH - Success\n",
            "-- Using _mm_prefetch\n",
            "-- Performing Test MM_MALLOC\n",
            "-- Performing Test MM_MALLOC - Success\n",
            "-- Using _mm_malloc\n",
            "-- Configuring done (6.7s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/LightGBM/build\n",
            "[  1%] Building CUDA object CMakeFiles/histo_16_64_256_sp_const.dir/src/treelearner/kernels/histogram_16_64_256.cu.o\n",
            "[  2%] Building CUDA object CMakeFiles/histo_16_64_256_sp.dir/src/treelearner/kernels/histogram_16_64_256.cu.o\n",
            "[  3%] Building CUDA object CMakeFiles/histo_16_64_256-allfeats_sp_const.dir/src/treelearner/kernels/histogram_16_64_256.cu.o\n",
            "[  5%] Building CUDA object CMakeFiles/histo_16_64_256-allfeats_sp.dir/src/treelearner/kernels/histogram_16_64_256.cu.o\n",
            "[  5%] Built target histo_16_64_256_sp_const\n",
            "[  5%] Built target histo_16_64_256-allfeats_sp_const\n",
            "[  5%] Built target histo_16_64_256_sp\n",
            "[  6%] Building CUDA object CMakeFiles/histo_16_64_256-fulldata_sp_const.dir/src/treelearner/kernels/histogram_16_64_256.cu.o\n",
            "[  7%] Building CUDA object CMakeFiles/histo_16_64_256-fulldata_sp.dir/src/treelearner/kernels/histogram_16_64_256.cu.o\n",
            "[  7%] Built target histo_16_64_256-allfeats_sp\n",
            "[  7%] Built target histo_16_64_256-fulldata_sp_const\n",
            "[  7%] Built target histo_16_64_256-fulldata_sp\n",
            "[  8%] Building CXX object CMakeFiles/lightgbm.dir/src/main.cpp.o\n",
            "[ 11%] Building CXX object CMakeFiles/_lightgbm.dir/src/boosting/boosting.cpp.o\n",
            "[ 11%] Building CXX object CMakeFiles/lightgbm.dir/src/application/application.cpp.o\n",
            "[ 12%] Building CXX object CMakeFiles/lightgbm.dir/src/boosting/boosting.cpp.o\n",
            "[ 13%] Building CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt.cpp.o\n",
            "[ 15%] Building CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt.cpp.o\n",
            "[ 16%] Building CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\n",
            "[ 17%] Building CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\n",
            "[ 18%] Building CXX object CMakeFiles/lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\n",
            "[ 20%] Building CXX object CMakeFiles/lightgbm.dir/src/io/bin.cpp.o\n",
            "[ 21%] Building CXX object CMakeFiles/lightgbm.dir/src/io/config.cpp.o\n",
            "[ 22%] Building CXX object CMakeFiles/lightgbm.dir/src/io/config_auto.cpp.o\n",
            "[ 24%] Building CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\n",
            "[ 25%] Building CXX object CMakeFiles/lightgbm.dir/src/io/dataset.cpp.o\n",
            "[ 26%] Building CXX object CMakeFiles/lightgbm.dir/src/io/dataset_loader.cpp.o\n",
            "[ 27%] Building CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\n",
            "[ 29%] Building CXX object CMakeFiles/_lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\n",
            "[ 30%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/bin.cpp.o\n",
            "[ 31%] Building CXX object CMakeFiles/lightgbm.dir/src/io/file_io.cpp.o\n",
            "[ 32%] Building CXX object CMakeFiles/lightgbm.dir/src/io/json11.cpp.o\n",
            "[ 34%] Building CXX object CMakeFiles/lightgbm.dir/src/io/metadata.cpp.o\n",
            "[ 35%] Building CXX object CMakeFiles/lightgbm.dir/src/io/parser.cpp.o\n",
            "[ 36%] Building CXX object CMakeFiles/lightgbm.dir/src/io/train_share_states.cpp.o\n",
            "[ 37%] Building CXX object CMakeFiles/lightgbm.dir/src/io/tree.cpp.o\n",
            "[ 39%] Building CXX object CMakeFiles/lightgbm.dir/src/metric/dcg_calculator.cpp.o\n",
            "[ 40%] Building CXX object CMakeFiles/lightgbm.dir/src/metric/metric.cpp.o\n",
            "[ 41%] Building CXX object CMakeFiles/lightgbm.dir/src/network/ifaddrs_patch.cpp.o\n",
            "[ 43%] Building CXX object CMakeFiles/lightgbm.dir/src/network/linker_topo.cpp.o\n",
            "[ 44%] Building CXX object CMakeFiles/lightgbm.dir/src/network/linkers_mpi.cpp.o\n",
            "[ 45%] Building CXX object CMakeFiles/lightgbm.dir/src/network/linkers_socket.cpp.o\n",
            "[ 46%] Building CXX object CMakeFiles/lightgbm.dir/src/network/network.cpp.o\n",
            "[ 48%] Building CXX object CMakeFiles/lightgbm.dir/src/objective/objective_function.cpp.o\n",
            "[ 49%] Building CUDA object CMakeFiles/lightgbm.dir/src/treelearner/cuda_kernel_launcher.cu.o\n",
            "[ 50%] Building CXX object CMakeFiles/lightgbm.dir/src/treelearner/cuda_tree_learner.cpp.o\n",
            "[ 51%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/config.cpp.o\n",
            "[ 53%] Building CXX object CMakeFiles/lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\n",
            "[ 54%] Building CXX object CMakeFiles/lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\n",
            "[ 55%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/config_auto.cpp.o\n",
            "[ 56%] Building CXX object CMakeFiles/lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\n",
            "[ 58%] Building CXX object CMakeFiles/lightgbm.dir/src/treelearner/linear_tree_learner.cpp.o\n",
            "[ 59%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/dataset.cpp.o\n",
            "[ 60%] Building CXX object CMakeFiles/lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\n",
            "[ 62%] Building CXX object CMakeFiles/lightgbm.dir/src/treelearner/tree_learner.cpp.o\n",
            "[ 63%] Building CXX object CMakeFiles/lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\n",
            "[ 64%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/dataset_loader.cpp.o\n",
            "[ 65%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/file_io.cpp.o\n",
            "[ 67%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/json11.cpp.o\n",
            "[ 68%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/metadata.cpp.o\n",
            "[ 69%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/parser.cpp.o\n",
            "[ 70%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/train_share_states.cpp.o\n",
            "[ 72%] Building CXX object CMakeFiles/_lightgbm.dir/src/io/tree.cpp.o\n",
            "[ 73%] Building CXX object CMakeFiles/_lightgbm.dir/src/metric/dcg_calculator.cpp.o\n",
            "[ 74%] Building CXX object CMakeFiles/_lightgbm.dir/src/metric/metric.cpp.o\n",
            "[ 75%] Building CXX object CMakeFiles/_lightgbm.dir/src/network/ifaddrs_patch.cpp.o\n",
            "[ 77%] Building CXX object CMakeFiles/_lightgbm.dir/src/network/linker_topo.cpp.o\n",
            "[ 78%] Building CXX object CMakeFiles/_lightgbm.dir/src/network/linkers_mpi.cpp.o\n",
            "[ 79%] Building CXX object CMakeFiles/_lightgbm.dir/src/network/linkers_socket.cpp.o\n",
            "[ 81%] Building CXX object CMakeFiles/_lightgbm.dir/src/network/network.cpp.o\n",
            "[ 82%] Building CXX object CMakeFiles/_lightgbm.dir/src/objective/objective_function.cpp.o\n",
            "[ 83%] Building CUDA object CMakeFiles/_lightgbm.dir/src/treelearner/cuda_kernel_launcher.cu.o\n",
            "[ 84%] Building CXX object CMakeFiles/_lightgbm.dir/src/treelearner/cuda_tree_learner.cpp.o\n",
            "[ 86%] Linking CUDA device code CMakeFiles/lightgbm.dir/cmake_device_link.o\n",
            "[ 87%] Linking CXX executable /content/LightGBM/lightgbm\n",
            "[ 87%] Built target lightgbm\n",
            "[ 88%] Building CXX object CMakeFiles/_lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\n",
            "[ 89%] Building CXX object CMakeFiles/_lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\n",
            "[ 91%] Building CXX object CMakeFiles/_lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\n",
            "[ 92%] Building CXX object CMakeFiles/_lightgbm.dir/src/treelearner/linear_tree_learner.cpp.o\n",
            "[ 93%] Building CXX object CMakeFiles/_lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\n",
            "[ 94%] Building CXX object CMakeFiles/_lightgbm.dir/src/treelearner/tree_learner.cpp.o\n",
            "[ 96%] Building CXX object CMakeFiles/_lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\n",
            "[ 97%] Building CXX object CMakeFiles/_lightgbm.dir/src/c_api.cpp.o\n",
            "[ 98%] Linking CUDA device code CMakeFiles/_lightgbm.dir/cmake_device_link.o\n",
            "[100%] Linking CXX shared library /content/LightGBM/lib_lightgbm.so\n",
            "[100%] Built target _lightgbm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Cloning into 'LightGBM'...\n",
            "Submodule 'include/boost/compute' (https://github.com/boostorg/compute) registered for path 'external_libs/compute'\n",
            "Submodule 'eigen' (https://gitlab.com/libeigen/eigen.git) registered for path 'external_libs/eigen'\n",
            "Submodule 'external_libs/fast_double_parser' (https://github.com/lemire/fast_double_parser.git) registered for path 'external_libs/fast_double_parser'\n",
            "Submodule 'external_libs/fmt' (https://github.com/fmtlib/fmt.git) registered for path 'external_libs/fmt'\n",
            "Cloning into '/content/LightGBM/external_libs/compute'...\n",
            "Cloning into '/content/LightGBM/external_libs/eigen'...\n",
            "Cloning into '/content/LightGBM/external_libs/fast_double_parser'...\n",
            "Cloning into '/content/LightGBM/external_libs/fmt'...\n",
            "Submodule 'benchmark/dependencies/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'\n",
            "Submodule 'benchmark/dependencies/double-conversion' (https://github.com/google/double-conversion.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'\n",
            "Cloning into '/content/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'...\n",
            "Cloning into '/content/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'...\n",
            "Switched to a new branch 'v3.3.2_gpu'\n",
            "CMake Warning (dev) at CMakeLists.txt:167 (find_package):\n",
            "  Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "  --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please consider removing the following classifiers in favor of a SPDX license expression:\n",
            "\n",
            "        License :: OSI Approved :: MIT License\n",
            "\n",
            "        See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self._finalize_license_expression()\n",
            "INFO:root:running install\n",
            "/content/LightGBM/python-package/setup.py:220: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  install.initialize_options(self)\n",
            "INFO:root:running build\n",
            "INFO:root:running build_py\n",
            "INFO:root:creating build/lib/lightgbm\n",
            "INFO:root:copying lightgbm/compat.py -> build/lib/lightgbm\n",
            "INFO:root:copying lightgbm/engine.py -> build/lib/lightgbm\n",
            "INFO:root:copying lightgbm/basic.py -> build/lib/lightgbm\n",
            "INFO:root:copying lightgbm/plotting.py -> build/lib/lightgbm\n",
            "INFO:root:copying lightgbm/libpath.py -> build/lib/lightgbm\n",
            "INFO:root:copying lightgbm/__init__.py -> build/lib/lightgbm\n",
            "INFO:root:copying lightgbm/dask.py -> build/lib/lightgbm\n",
            "INFO:root:copying lightgbm/sklearn.py -> build/lib/lightgbm\n",
            "INFO:root:copying lightgbm/callback.py -> build/lib/lightgbm\n",
            "INFO:root:running egg_info\n",
            "INFO:root:creating lightgbm.egg-info\n",
            "INFO:root:writing lightgbm.egg-info/PKG-INFO\n",
            "INFO:root:writing dependency_links to lightgbm.egg-info/dependency_links.txt\n",
            "INFO:root:writing requirements to lightgbm.egg-info/requires.txt\n",
            "INFO:root:writing top-level names to lightgbm.egg-info/top_level.txt\n",
            "INFO:root:writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "INFO:root:reading manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "INFO:root:reading manifest template 'MANIFEST.in'\n",
            "WARNING:root:no previously-included directories found matching 'build'\n",
            "WARNING:root:warning: no files found matching 'LICENSE'\n",
            "WARNING:root:warning: no files found matching '*.txt'\n",
            "WARNING:root:warning: no files found matching '*.so' under directory 'lightgbm'\n",
            "WARNING:root:warning: no files found matching 'compile/CMakeLists.txt'\n",
            "WARNING:root:warning: no files found matching 'compile/cmake/IntegratedOpenCL.cmake'\n",
            "WARNING:root:warning: no files found matching '*.so' under directory 'compile'\n",
            "WARNING:root:warning: no files found matching '*.dll' under directory 'compile/Release'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/compute/CMakeLists.txt'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/compute/cmake'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/compute/include'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/compute/meta'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/CMakeLists.txt'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/Eigen/Cholesky'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/Eigen/Core'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/Eigen/Dense'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/Eigen/Eigenvalues'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/Eigen/Geometry'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/Eigen/Householder'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/Eigen/Jacobi'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/Eigen/LU'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/Eigen/QR'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/eigen/Eigen/SVD'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Cholesky'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Core'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Eigenvalues'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Geometry'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Householder'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Jacobi'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/LU'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/misc'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/plugins'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/QR'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/SVD'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/fast_double_parser/CMakeLists.txt'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE.BSL'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/fast_double_parser/include'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/fmt/CMakeLists.txt'\n",
            "WARNING:root:warning: no files found matching 'compile/external_libs/fmt/LICENSE.rst'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/external_libs/fmt/include'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/include'\n",
            "WARNING:root:warning: no files found matching '*' under directory 'compile/src'\n",
            "WARNING:root:warning: no files found matching 'LightGBM.sln' under directory 'compile/windows'\n",
            "WARNING:root:warning: no files found matching 'LightGBM.vcxproj' under directory 'compile/windows'\n",
            "WARNING:root:warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n",
            "WARNING:root:warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
            "WARNING:root:warning: no previously-included files found matching 'compile/external_libs/compute/.git'\n",
            "INFO:root:writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "INFO:root:copying lightgbm/VERSION.txt -> build/lib/lightgbm\n",
            "INFO:root:running install_lib\n",
            "INFO:root:copying build/lib/lightgbm/compat.py -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:root:copying build/lib/lightgbm/engine.py -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:root:copying build/lib/lightgbm/basic.py -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:root:copying build/lib/lightgbm/plotting.py -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:root:copying build/lib/lightgbm/libpath.py -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:root:copying build/lib/lightgbm/__init__.py -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:root:copying build/lib/lightgbm/dask.py -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:root:copying build/lib/lightgbm/sklearn.py -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:root:copying build/lib/lightgbm/callback.py -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:root:copying build/lib/lightgbm/VERSION.txt -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:LightGBM:Installing lib_lightgbm from: ['/content/LightGBM/lib_lightgbm.so']\n",
            "INFO:root:copying /content/LightGBM/lib_lightgbm.so -> /usr/local/lib/python3.11/dist-packages/lightgbm\n",
            "INFO:root:byte-compiling /usr/local/lib/python3.11/dist-packages/lightgbm/compat.py to compat.cpython-311.pyc\n",
            "INFO:root:byte-compiling /usr/local/lib/python3.11/dist-packages/lightgbm/engine.py to engine.cpython-311.pyc\n",
            "INFO:root:byte-compiling /usr/local/lib/python3.11/dist-packages/lightgbm/basic.py to basic.cpython-311.pyc\n",
            "INFO:root:byte-compiling /usr/local/lib/python3.11/dist-packages/lightgbm/plotting.py to plotting.cpython-311.pyc\n",
            "INFO:root:byte-compiling /usr/local/lib/python3.11/dist-packages/lightgbm/libpath.py to libpath.cpython-311.pyc\n",
            "INFO:root:byte-compiling /usr/local/lib/python3.11/dist-packages/lightgbm/__init__.py to __init__.cpython-311.pyc\n",
            "INFO:root:byte-compiling /usr/local/lib/python3.11/dist-packages/lightgbm/dask.py to dask.cpython-311.pyc\n",
            "INFO:root:byte-compiling /usr/local/lib/python3.11/dist-packages/lightgbm/sklearn.py to sklearn.cpython-311.pyc\n",
            "INFO:root:byte-compiling /usr/local/lib/python3.11/dist-packages/lightgbm/callback.py to callback.cpython-311.pyc\n",
            "INFO:root:running install_egg_info\n",
            "INFO:root:Copying lightgbm.egg-info to /usr/local/lib/python3.11/dist-packages/lightgbm-3.3.2-py3.11.egg-info\n",
            "INFO:root:running install_scripts\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# 1. 시스템 의존성 설치 (CMake, Boost 등)\n",
        "apt-get update && apt-get install -y cmake libboost-dev libboost-system-dev libboost-filesystem-dev\n",
        "\n",
        "# 2. 기존 LightGBM 폴더 삭제 후 GitHub에서 클론\n",
        "rm -rf LightGBM\n",
        "git clone --recursive https://github.com/microsoft/LightGBM.git\n",
        "\n",
        "# 3. v3.3.2 태그로 체크아웃 (setup.py 기반 버전)\n",
        "cd LightGBM\n",
        "git checkout tags/v3.3.2 -b v3.3.2_gpu\n",
        "\n",
        "# 4. CUDA 지원 빌드 진행\n",
        "mkdir build && cd build\n",
        "cmake -DUSE_CUDA=ON ..\n",
        "make -j4\n",
        "\n",
        "# 5. Python 바인딩 설치 (setup.py를 사용하여 precompile 옵션으로 설치)\n",
        "cd ../python-package\n",
        "python setup.py install --precompile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPZUwJOt04na",
        "outputId": "79dbbbce-f456-492d-e381-0efb764aedcf",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ba8ab56-86f5-4aa9-a7d6-c760a9463c1a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6ba8ab56-86f5-4aa9-a7d6-c760a9463c1a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 소상공인시장진흥공단_상가(상권)정보_경북_202412.csv to 소상공인시장진흥공단_상가(상권)정보_경북_202412.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # CSV 파일 업로드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZpwVvxs04px",
        "collapsed": true,
        "outputId": "c86f2f45-3793-4c66-d51c-f1f4ca7c762e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/4] 데이터 로드 중...\n",
            "원본 데이터 크기: (147012, 39)\n",
            "[2/4] 상호명 전처리 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 147012/147012 [01:18<00:00, 1862.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3/4] 카테고리 매핑 중...\n",
            "전처리 완료! (preprocessed_data.csv 생성)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from joblib import dump\n",
        "from tqdm import tqdm\n",
        "\n",
        "# tqdm의 pandas 확장 활성화\n",
        "tqdm.pandas()\n",
        "\n",
        "# CSV 데이터 로드\n",
        "print(\"[1/4] 데이터 로드 중...\")\n",
        "df = pd.read_csv(\"소상공인시장진흥공단_상가(상권)정보_경북_202412.csv\")\n",
        "print(f\"원본 데이터 크기: {df.shape}\")\n",
        "\n",
        "# 형태소 분석기 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 전처리 함수: 소문자화, 특수문자 제거, 명사 추출\n",
        "def enhanced_preprocess(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text)\n",
        "    nouns = okt.nouns(text)\n",
        "    return ' '.join(nouns).strip()\n",
        "\n",
        "# 상호명 전처리 진행 (tqdm으로 진행률 확인)\n",
        "print(\"[2/4] 상호명 전처리 중...\")\n",
        "df[\"상호명_전처리\"] = df[\"상호명\"].progress_apply(enhanced_preprocess)\n",
        "\n",
        "# 8개 메인 카테고리 매핑 (원하는 코드에 맞게 수정)\n",
        "category_mapping = {\n",
        "    \"식비\": [\"I201\", \"I202\", \"I203\", \"I204\", \"I205\", \"I206\", \"I207\",\n",
        "           \"I210\", \"I211\", \"I212\", \"G204\", \"G205\", \"G206\"],\n",
        "    \"교통\": [\"G214\", \"N109\", \"S203\"],\n",
        "    \"주거\": [\"L102\", \"D101\", \"D102\", \"D103\"],\n",
        "    \"의료\": [\"Q101\", \"Q102\", \"Q104\", \"G215\", \"M111\"],\n",
        "    \"문화\": [\"R102\", \"R103\", \"R104\", \"I101\", \"I102\", \"N110\"],\n",
        "    \"쇼핑\": [\"G202\", \"G203\", \"G208\", \"G209\", \"G210\", \"G211\", \"G212\",\n",
        "             \"G216\", \"G217\", \"G218\", \"G219\", \"G220\", \"G221\", \"G222\"],\n",
        "    \"교육\": [\"P105\", \"P106\", \"P107\", \"G213\"],\n",
        "    \"기타\": [\"M103\", \"M104\", \"M105\", \"M106\", \"M107\", \"M109\", \"M112\",\n",
        "           \"M113\", \"M114\", \"M115\", \"N101\", \"N102\", \"N103\", \"N104\",\n",
        "           \"N105\", \"N107\", \"N108\", \"N111\", \"S201\", \"S202\", \"S204\",\n",
        "           \"S205\", \"S206\", \"S207\", \"S208\", \"S209\", \"S210\", \"S211\", \"G207\"]\n",
        "}\n",
        "\n",
        "# 코드별 카테고리 매핑\n",
        "print(\"[3/4] 카테고리 매핑 중...\")\n",
        "code_to_category = {code: cat for cat, codes in category_mapping.items() for code in codes}\n",
        "df[\"카테고리\"] = df[\"상권업종중분류코드\"].astype(str).map(lambda x: code_to_category.get(x, \"기타\"))\n",
        "\n",
        "# 전처리 결과 저장 (추후 모델 학습에 사용)\n",
        "df.to_csv(\"preprocessed_data.csv\", index=False)\n",
        "dump(code_to_category, \"category_mapping.joblib\")\n",
        "print(\"전처리 완료! (preprocessed_data.csv 생성)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzfFk0QB04sh",
        "outputId": "00ba7ec5-d12e-4917-bf36-610a248e605d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 147012/147012 [00:31<00:00, 4726.85it/s]\n",
            "[I 2025-04-04 04:40:16,882] A new study created in memory with name: no-name-04d621c9-91d8-4b01-8d16-f622585391a2\n",
            "[I 2025-04-04 04:42:00,959] Trial 0 finished with value: 0.7527170367949736 and parameters: {'analyzer': 'char', 'ngram_upper': 2, 'sampler': 'none', 'eta': 0.04777742998984467, 'max_depth': 7, 'gamma': 2.8223085590251045, 'min_child_weight': 4, 'subsample': 0.8723606974982616, 'colsample_bytree': 0.9511946509113967, 'reg_alpha': 1.3372191429854585e-05, 'reg_lambda': 1.6433391341350338e-07, 'num_boost_round': 419}. Best is trial 0 with value: 0.7527170367949736.\n",
            "[I 2025-04-04 04:42:56,438] Trial 1 finished with value: 0.6660056019566613 and parameters: {'analyzer': 'char', 'ngram_upper': 2, 'sampler': 'none', 'eta': 0.025996792571438866, 'max_depth': 7, 'gamma': 4.922976123066266, 'min_child_weight': 6, 'subsample': 0.9622247115889808, 'colsample_bytree': 0.7704380064279576, 'reg_alpha': 0.0004704806415725675, 'reg_lambda': 8.327607726579723e-06, 'num_boost_round': 141}. Best is trial 0 with value: 0.7527170367949736.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "[I 2025-04-04 04:43:17,975] Trial 2 finished with value: 0.574176692034095 and parameters: {'analyzer': 'word', 'ngram_upper': 3, 'sampler': 'none', 'eta': 0.001607549352816882, 'max_depth': 7, 'gamma': 3.53440057403847, 'min_child_weight': 5, 'subsample': 0.9762760833663946, 'colsample_bytree': 0.8834779020776493, 'reg_alpha': 0.00023091279896500483, 'reg_lambda': 9.920207565094447, 'num_boost_round': 291}. Best is trial 0 with value: 0.7527170367949736.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "[I 2025-04-04 04:46:48,641] Trial 3 finished with value: 0.53900791727693 and parameters: {'analyzer': 'word', 'ngram_upper': 2, 'sampler': 'adasyn', 'eta': 0.004806923676560357, 'max_depth': 12, 'gamma': 0.9345616198397932, 'min_child_weight': 5, 'subsample': 0.9053849725056592, 'colsample_bytree': 0.8588824865910012, 'reg_alpha': 0.019076027204933087, 'reg_lambda': 3.9521147278970544e-07, 'num_boost_round': 107}. Best is trial 0 with value: 0.7527170367949736.\n",
            "[I 2025-04-04 04:51:20,939] Trial 4 finished with value: 0.7805327439544726 and parameters: {'analyzer': 'char', 'ngram_upper': 3, 'sampler': 'none', 'eta': 0.0822075335012693, 'max_depth': 15, 'gamma': 0.39553296799113347, 'min_child_weight': 3, 'subsample': 0.7613705543277253, 'colsample_bytree': 0.7084088713502827, 'reg_alpha': 1.4675569643363483e-05, 'reg_lambda': 0.03529364799649397, 'num_boost_round': 278}. Best is trial 4 with value: 0.7805327439544726.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "[I 2025-04-04 04:52:58,112] Trial 5 finished with value: 0.5284956017169623 and parameters: {'analyzer': 'word', 'ngram_upper': 2, 'sampler': 'smote', 'eta': 0.002515909261880049, 'max_depth': 9, 'gamma': 0.537321208848629, 'min_child_weight': 7, 'subsample': 0.8726653197528871, 'colsample_bytree': 0.9433981627515392, 'reg_alpha': 5.814046568932079e-07, 'reg_lambda': 9.73689008135246e-06, 'num_boost_round': 261}. Best is trial 4 with value: 0.7805327439544726.\n",
            "[I 2025-04-04 05:07:18,771] Trial 6 finished with value: 0.684957775791317 and parameters: {'analyzer': 'char', 'ngram_upper': 3, 'sampler': 'adasyn', 'eta': 0.011241258191209564, 'max_depth': 8, 'gamma': 0.09813777615585662, 'min_child_weight': 4, 'subsample': 0.9977960429228381, 'colsample_bytree': 0.9295181993346384, 'reg_alpha': 2.8487978634035534e-05, 'reg_lambda': 3.526808301563799e-05, 'num_boost_round': 365}. Best is trial 4 with value: 0.7805327439544726.\n",
            "[I 2025-04-04 05:14:40,171] Trial 7 finished with value: 0.7628097792425153 and parameters: {'analyzer': 'char', 'ngram_upper': 3, 'sampler': 'none', 'eta': 0.027162356210131203, 'max_depth': 15, 'gamma': 0.9411790688716731, 'min_child_weight': 2, 'subsample': 0.754558076074775, 'colsample_bytree': 0.8100184875076782, 'reg_alpha': 1.3412970416255675e-08, 'reg_lambda': 3.2440715236937646e-06, 'num_boost_round': 382}. Best is trial 4 with value: 0.7805327439544726.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "[I 2025-04-04 05:15:27,960] Trial 8 finished with value: 0.6646145591953116 and parameters: {'analyzer': 'word', 'ngram_upper': 3, 'sampler': 'smote', 'eta': 0.06165156136707987, 'max_depth': 7, 'gamma': 3.4000437494170717, 'min_child_weight': 4, 'subsample': 0.9535897144565426, 'colsample_bytree': 0.8302768838568275, 'reg_alpha': 3.211102235371509e-07, 'reg_lambda': 1.1968728082622123e-07, 'num_boost_round': 167}. Best is trial 4 with value: 0.7805327439544726.\n",
            "[I 2025-04-04 05:22:59,434] Trial 9 finished with value: 0.6558391112132109 and parameters: {'analyzer': 'char', 'ngram_upper': 2, 'sampler': 'smote', 'eta': 0.0029978160153789936, 'max_depth': 12, 'gamma': 1.3039482087649117, 'min_child_weight': 4, 'subsample': 0.8975877344356618, 'colsample_bytree': 0.7354701477157721, 'reg_alpha': 2.01292573615169e-07, 'reg_lambda': 7.432571921887297e-07, 'num_boost_round': 322}. Best is trial 4 with value: 0.7805327439544726.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Optuna] Best score: 0.7805327439544726\n",
            "[Optuna] Best params: {'analyzer': 'char', 'ngram_upper': 3, 'sampler': 'none', 'eta': 0.0822075335012693, 'max_depth': 15, 'gamma': 0.39553296799113347, 'min_child_weight': 3, 'subsample': 0.7613705543277253, 'colsample_bytree': 0.7084088713502827, 'reg_alpha': 1.4675569643363483e-05, 'reg_lambda': 0.03529364799649397, 'num_boost_round': 278}\n",
            "\n",
            "[최종 성능 평가]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          교육       0.86      0.63      0.73      1554\n",
            "          교통       0.89      0.77      0.82      1249\n",
            "          기타       0.85      0.73      0.78      5423\n",
            "          문화       0.88      0.62      0.73      2474\n",
            "          쇼핑       0.73      0.42      0.54      3552\n",
            "          식비       0.75      0.97      0.85     13473\n",
            "          의료       0.92      0.66      0.77       869\n",
            "          주거       0.98      0.91      0.94       809\n",
            "\n",
            "    accuracy                           0.79     29403\n",
            "   macro avg       0.86      0.71      0.77     29403\n",
            "weighted avg       0.80      0.79      0.78     29403\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:27:35] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최적 모델 및 벡터라이저, 라벨 인코더 저장 완료!\n"
          ]
        }
      ],
      "source": [
        "##############################\n",
        "# 0) 라이브러리 & 설치 (Colab 경우)\n",
        "##############################\n",
        "# 필요 시 아래 명령어 실행 (런타임 재시작 후 진행 권장)\n",
        "# !pip install --upgrade pip setuptools wheel\n",
        "# !pip install pandas scikit-learn joblib fastapi uvicorn nest-asyncio optuna konlpy imbalanced-learn\n",
        "# !pip install JPype1==1.4.1  # konlpy 호환\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import scipy.sparse\n",
        "import joblib\n",
        "import optuna\n",
        "from tqdm import tqdm\n",
        "\n",
        "##############################\n",
        "# 1) konlpy, imblearn 및 기타 임포트\n",
        "##############################\n",
        "from konlpy.tag import Okt\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "\n",
        "##############################\n",
        "# 2) 데이터 로드 및 기본 전처리\n",
        "##############################\n",
        "# \"preprocessed_data.csv\" 파일에 [\"상호명_전처리\", \"카테고리\", \"상호명\"] 컬럼이 있다고 가정\n",
        "df = pd.read_csv(\"preprocessed_data.csv\", usecols=[\"상호명_전처리\", \"카테고리\", \"상호명\"])\n",
        "df[\"상호명_전처리\"] = df[\"상호명_전처리\"].fillna(\"\").astype(str)\n",
        "\n",
        "##############################\n",
        "# 3) 명사 추출 함수 (전처리)\n",
        "##############################\n",
        "okt = Okt()\n",
        "def only_nouns(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^가-힣0-9a-z\\s]\", \"\", text)\n",
        "    nouns = okt.nouns(text)\n",
        "    stopwords = {\"점\", \"센터\", \"코너\", \"플라자\", \"주식회사\", \"유한회사\"}\n",
        "    tokens = [w for w in nouns if w not in stopwords and len(w) > 1]\n",
        "    return \" \".join(tokens).strip()\n",
        "\n",
        "# 이미 어느 정도 전처리된 컬럼이지만, 일관성을 위해 재적용\n",
        "tqdm.pandas()\n",
        "df[\"상호명_전처리\"] = df[\"상호명_전처리\"].progress_apply(only_nouns)\n",
        "\n",
        "##############################\n",
        "# 4) 쇼핑 키워드 피처 추가\n",
        "##############################\n",
        "# 쇼핑 관련 대표 키워드 목록 (필요에 따라 수정)\n",
        "shopping_keywords = [\"마트\", \"편의점\", \"백화점\", \"쇼핑\", \"문구\", \"의류\", \"화장품\", \"가전\", \"가구\", \"서점\"]\n",
        "\n",
        "def has_shop_keyword(text: str) -> int:\n",
        "    # 원본 상호명을 활용하거나 전처리 후 상호명을 활용할 수 있음.\n",
        "    # 여기서는 원본 상호명을 사용하여, 영문 및 특수문자 등도 고려.\n",
        "    text = text.lower()\n",
        "    for kw in shopping_keywords:\n",
        "        if kw in text:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "# 추가 피처: 쇼핑 관련 키워드 포함 여부\n",
        "df[\"has_shop_kw\"] = df[\"상호명\"].apply(has_shop_keyword)\n",
        "\n",
        "##############################\n",
        "# 5) 라벨 인코딩\n",
        "##############################\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[\"카테고리\"])\n",
        "\n",
        "##############################\n",
        "# 6) 데이터 분할\n",
        "##############################\n",
        "# 텍스트 데이터 (전처리된 상호명)와 추가 피처 (has_shop_kw) 분리\n",
        "X_text_raw = df[\"상호명_전처리\"].values  # 문자열 배열\n",
        "X_kw = df[\"has_shop_kw\"].values.reshape(-1, 1)  # 수치형 배열\n",
        "\n",
        "# 먼저 train/test 분할 (동일 인덱스 유지)\n",
        "X_text_train, X_text_test, y_train, y_test, X_kw_train, X_kw_test = train_test_split(\n",
        "    X_text_raw, y, X_kw, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "##############################\n",
        "# 7) Optuna 탐색 함수 정의 (키워드 피처 포함)\n",
        "##############################\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def objective(trial):\n",
        "    # --- 7.1) TF-IDF 설정 ---\n",
        "    analyzer_choice = trial.suggest_categorical(\"analyzer\", [\"word\", \"char\"])\n",
        "    ngram_lower = 1\n",
        "    ngram_upper = trial.suggest_int(\"ngram_upper\", 2, 3)  # 예: (1,2) 또는 (1,3)\n",
        "\n",
        "    if analyzer_choice == \"word\":\n",
        "        # 단어 기반: 이미 only_nouns()로 토큰화되어 있으므로 단순 split 사용\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            tokenizer=lambda x: x.split(),\n",
        "            analyzer=\"word\",\n",
        "            ngram_range=(ngram_lower, ngram_upper),\n",
        "            min_df=3, max_df=0.85,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "    else:\n",
        "        # 문자 기반: 전처리 함수 적용 후 문자 n-그램 생성\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            preprocessor=only_nouns,\n",
        "            analyzer=\"char_wb\",\n",
        "            ngram_range=(ngram_lower, ngram_upper),\n",
        "            min_df=3, max_df=0.85,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_text_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_text_test)\n",
        "\n",
        "    # --- 7.2) 키워드 피처 결합 ---\n",
        "    # 추가 피처는 dense이므로 sparse 변환 후 hstack\n",
        "    from scipy.sparse import csr_matrix, hstack\n",
        "    X_kw_train_sparse = csr_matrix(X_kw_train)\n",
        "    X_kw_test_sparse = csr_matrix(X_kw_test)\n",
        "\n",
        "    X_train_all = hstack([X_train_tfidf, X_kw_train_sparse])\n",
        "    X_test_all  = hstack([X_test_tfidf, X_kw_test_sparse])\n",
        "\n",
        "    # --- 7.3) 오버샘플링 방법 ---\n",
        "    sampler_choice = trial.suggest_categorical(\"sampler\", [\"none\", \"smote\", \"adasyn\"])\n",
        "    if sampler_choice == \"smote\":\n",
        "        sampler = SMOTE(random_state=42)\n",
        "        X_res, y_res = sampler.fit_resample(X_train_all, y_train)\n",
        "    elif sampler_choice == \"adasyn\":\n",
        "        sampler = ADASYN(random_state=42)\n",
        "        X_res, y_res = sampler.fit_resample(X_train_all, y_train)\n",
        "    else:\n",
        "        X_res, y_res = X_train_all, y_train\n",
        "\n",
        "    # --- 7.4) XGBoost 하이퍼파라미터 ---\n",
        "    param = {\n",
        "        \"objective\": \"multi:softmax\",\n",
        "        \"num_class\": len(le.classes_),\n",
        "        \"tree_method\": \"hist\",   # XGBoost 2.0 이상 권장 (hist + device)\n",
        "        \"device\": \"cuda\",\n",
        "        \"eval_metric\": \"mlogloss\",  # 조기종료 기준용\n",
        "        \"eta\": trial.suggest_float(\"eta\", 1e-3, 1e-1, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 16),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 8),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 1.0),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
        "    }\n",
        "    num_boost_round = trial.suggest_int(\"num_boost_round\", 100, 500)\n",
        "\n",
        "    # --- 7.5) XGBoost 학습 ---\n",
        "    dtrain = xgb.DMatrix(X_res, label=y_res)\n",
        "    dtest  = xgb.DMatrix(X_test_all, label=y_test)\n",
        "    watchlist = [(dtrain, \"train\"), (dtest, \"eval\")]\n",
        "    booster = xgb.train(\n",
        "        params=param,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        evals=watchlist,\n",
        "        early_stopping_rounds=10,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "\n",
        "    best_iter = booster.best_iteration\n",
        "    y_pred = booster.predict(dtest, iteration_range=(0, best_iter+1))\n",
        "\n",
        "    return f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "##############################\n",
        "# 8) Optuna 탐색 실행\n",
        "##############################\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)  # n_trials는 시간 여유에 따라 조정\n",
        "\n",
        "print(\"[Optuna] Best score:\", study.best_trial.value)\n",
        "print(\"[Optuna] Best params:\", study.best_trial.params)\n",
        "\n",
        "##############################\n",
        "# 9) 최적 파라미터로 최종 재학습\n",
        "##############################\n",
        "best_params = study.best_params\n",
        "\n",
        "# --- 9.1) Vectorizer 재생성 ---\n",
        "if best_params[\"analyzer\"] == \"word\":\n",
        "    final_vectorizer = TfidfVectorizer(\n",
        "        tokenizer=lambda x: x.split(),\n",
        "        analyzer=\"word\",\n",
        "        ngram_range=(1, best_params[\"ngram_upper\"]),\n",
        "        min_df=3, max_df=0.85, dtype=np.float32\n",
        "    )\n",
        "else:\n",
        "    final_vectorizer = TfidfVectorizer(\n",
        "        preprocessor=only_nouns,\n",
        "        analyzer=\"char_wb\",\n",
        "        ngram_range=(1, best_params[\"ngram_upper\"]),\n",
        "        min_df=3, max_df=0.85, dtype=np.float32\n",
        "    )\n",
        "X_train_tfidf = final_vectorizer.fit_transform(X_text_train)\n",
        "X_test_tfidf  = final_vectorizer.transform(X_text_test)\n",
        "\n",
        "# --- 9.2) 키워드 피처 결합 ---\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "X_kw_train_sparse = csr_matrix(X_kw_train)\n",
        "X_kw_test_sparse  = csr_matrix(X_kw_test)\n",
        "X_train_all = hstack([X_train_tfidf, X_kw_train_sparse])\n",
        "X_test_all  = hstack([X_test_tfidf, X_kw_test_sparse])\n",
        "\n",
        "# --- 9.3) 오버샘플링 최종 적용 ---\n",
        "sampler_choice = best_params[\"sampler\"]\n",
        "if sampler_choice == \"smote\":\n",
        "    sampler = SMOTE(random_state=42)\n",
        "    X_res, y_res = sampler.fit_resample(X_train_all, y_train)\n",
        "elif sampler_choice == \"adasyn\":\n",
        "    sampler = ADASYN(random_state=42)\n",
        "    X_res, y_res = sampler.fit_resample(X_train_all, y_train)\n",
        "else:\n",
        "    X_res, y_res = X_train_all, y_train\n",
        "\n",
        "# --- 9.4) 최적 XGBoost 파라미터 적용 ---\n",
        "param_final = {\n",
        "    \"objective\": \"multi:softmax\",\n",
        "    \"num_class\": len(le.classes_),\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"device\": \"cuda\",\n",
        "    \"eval_metric\": \"mlogloss\",\n",
        "    \"eta\": best_params[\"eta\"],\n",
        "    \"max_depth\": best_params[\"max_depth\"],\n",
        "    \"gamma\": best_params[\"gamma\"],\n",
        "    \"min_child_weight\": best_params[\"min_child_weight\"],\n",
        "    \"subsample\": best_params[\"subsample\"],\n",
        "    \"colsample_bytree\": best_params[\"colsample_bytree\"],\n",
        "    \"reg_alpha\": best_params[\"reg_alpha\"],\n",
        "    \"reg_lambda\": best_params[\"reg_lambda\"],\n",
        "}\n",
        "final_num_boost_round = best_params[\"num_boost_round\"]\n",
        "\n",
        "dtrain_final = xgb.DMatrix(X_res, label=y_res)\n",
        "dtest_final  = xgb.DMatrix(X_test_all, label=y_test)\n",
        "watchlist_final = [(dtrain_final, \"train\"), (dtest_final, \"eval\")]\n",
        "\n",
        "final_booster = xgb.train(\n",
        "    params=param_final,\n",
        "    dtrain=dtrain_final,\n",
        "    num_boost_round=final_num_boost_round,\n",
        "    evals=watchlist_final,\n",
        "    early_stopping_rounds=10,\n",
        "    verbose_eval=False\n",
        ")\n",
        "\n",
        "best_iter = final_booster.best_iteration\n",
        "y_pred = final_booster.predict(dtest_final, iteration_range=(0, best_iter+1))\n",
        "\n",
        "##############################\n",
        "# 10) 최종 성능 평가 및 저장\n",
        "##############################\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\n[최종 성능 평가]\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "final_booster.save_model(\"best_booster.model\")\n",
        "joblib.dump(final_vectorizer, \"final_vectorizer.joblib\")\n",
        "joblib.dump(le, \"label_encoder.joblib\")\n",
        "print(\"최적 모델 및 벡터라이저, 라벨 인코더 저장 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6QDzxQd04u4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39444e5-6641-42aa-d80f-9e889df7daf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:33:11] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델, 벡터라이저, 라벨 인코더 저장 완료!\n",
            "상호명: 하이내과 -> 예측 카테고리: 의료\n"
          ]
        }
      ],
      "source": [
        "from joblib import dump, load\n",
        "import dill\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "import xgboost as xgb\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "# 학습 시 사용한 전처리 함수와 동일하게 정의\n",
        "def only_nouns(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^가-힣0-9a-z\\s]\", \"\", text)\n",
        "    nouns = okt.nouns(text)\n",
        "    stopwords = {\"점\", \"센터\", \"코너\", \"플라자\", \"주식회사\", \"유한회사\"}\n",
        "    tokens = [w for w in nouns if w not in stopwords and len(w) > 1]\n",
        "    return \" \".join(tokens).strip()\n",
        "\n",
        "# 모델 저장 (학습 완료된 Booster 객체를 final_booster라고 가정)\n",
        "final_booster.save_model(\"best_booster.model\")\n",
        "dill.dump(final_vectorizer, open(\"final_vectorizer.joblib\", \"wb\"))\n",
        "dill.dump(le, open(\"label_encoder.joblib\", \"wb\"))\n",
        "print(\"모델, 벡터라이저, 라벨 인코더 저장 완료!\")\n",
        "\n",
        "# 단일 예측 함수: 전처리 단계에 only_nouns 함수 사용\n",
        "def predict_category(store_name):\n",
        "    store_name_processed = only_nouns(store_name)\n",
        "    X_new = final_vectorizer.transform([store_name_processed])\n",
        "    dmatrix = xgb.DMatrix(X_new)\n",
        "    pred = final_booster.predict(dmatrix)\n",
        "    return le.inverse_transform([int(pred[0])])[0]\n",
        "\n",
        "# 예측 테스트\n",
        "test_input = \"하이내과\"\n",
        "print(\"상호명:\", test_input, \"-> 예측 카테고리:\", predict_category(test_input))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치 (Colab 런타임 재시작 후 실행)\n",
        "!pip install dill nest-asyncio pyngrok fastapi uvicorn\n",
        "\n",
        "import dill\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "import xgboost as xgb\n",
        "\n",
        "# 1. 저장된 모델 및 전처리기 로드 (Dill 사용)\n",
        "with open('final_vectorizer.joblib', 'rb') as f:\n",
        "    final_vectorizer = dill.load(f)\n",
        "with open('label_encoder.joblib', 'rb') as f:\n",
        "    le = dill.load(f)\n",
        "\n",
        "booster = xgb.Booster()\n",
        "booster.load_model('best_booster.model')\n",
        "\n",
        "# 2. 전처리 함수 (학습시와 동일)\n",
        "okt = Okt()\n",
        "def only_nouns(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^가-힣0-9a-z\\s]\", \"\", text)\n",
        "    nouns = okt.nouns(text)\n",
        "    stopwords = {\"점\", \"센터\", \"코너\", \"플라자\", \"주식회사\", \"유한회사\"}\n",
        "    tokens = [w for w in nouns if w not in stopwords and len(w) > 1]\n",
        "    return \" \".join(tokens).strip()\n",
        "\n",
        "# 3. FastAPI 앱 설정\n",
        "app = FastAPI()\n",
        "\n",
        "class StoreRequest(BaseModel):\n",
        "    store_name: str\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(request: StoreRequest):\n",
        "    processed_text = only_nouns(request.store_name)\n",
        "    X = final_vectorizer.transform([processed_text])\n",
        "    dmatrix = xgb.DMatrix(X)\n",
        "    pred = booster.predict(dmatrix)\n",
        "    return {\n",
        "        \"store_name\": request.store_name,\n",
        "        \"category\": le.inverse_transform([int(pred[0])])[0]\n",
        "    }\n",
        "\n",
        "# 4. Colab에서 서버 실행\n",
        "nest_asyncio.apply()\n",
        "public_url = ngrok.connect(8001).public_url\n",
        "print(f\"🔥 서버 실행 중! 접속 URL: {public_url}\")\n",
        "\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUQlA55etaU_",
        "outputId": "4a27f1b3-fb52-46c6-ee60-87cfd44077ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (0.3.9)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "🔥 서버 실행 중! 접속 URL: https://ab6b-35-240-156-58.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [3359]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     14.46.141.248:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     14.46.141.248:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     14.46.141.248:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     14.46.141.248:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [3359]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 모든 파일을 zip으로 압축\n",
        "!zip model_files.zip best_booster.model *.joblib\n",
        "\n",
        "# 2. 압축 파일 다운로드\n",
        "from google.colab import files\n",
        "files.download('model_files.zip')"
      ],
      "metadata": {
        "id": "QAYGRWflv60f",
        "outputId": "0cb9ef0b-5e53-4630-a888-29d00889a1a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: best_booster.model (deflated 57%)\n",
            "  adding: category_mapping.joblib (deflated 59%)\n",
            "  adding: final_vectorizer.joblib (deflated 55%)\n",
            "  adding: label_encoder.joblib (deflated 15%)\n",
            "  adding: vectorizer.joblib (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9d724ab6-34c2-470b-a929-8ac19ec84d79\", \"model_files.zip\", 3911447)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}